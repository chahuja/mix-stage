<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="UTF-8">
  <title> Style Transfer for Co-Speech Gesture Animation: A Multi-Speaker Conditional-Mixture Approach by chahuja</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
  <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  <link href="assets/css/style2.css" rel="stylesheet" type="text/css">
  <link href="assets/css/style.css" rel="stylesheet" type="text/css">
</head>
<body style="background-color: white;">
  <section class="page-header" >
    <h1 class="project-name" style="font-size: 2.4rem"> Style Transfer for Co-Speech Gesture Animation: <br>A Multi-Speaker Conditional-Mixture Approach</h1>
    <h2 class="project-tagline"><a href="http://chahuja.com">Chaitanya Ahuja</a>, Dong Won Lee, Yukiko I. Nakano, <a href="https://www.cs.cmu.edu/~morency/">Louis-Philippe Morency</a></h2>
    <h2 class="project-tagline">ECCV 2020</h2>
    <a href="https://arxiv.org/abs/2007.12553"class="button">Paper</a> 
    <a href="https://github.com/chahuja/mix-stage"class="button">Code</a>
    <a href="http://chahuja.com/pats" class="button">Dataset Website</a> 
    <a href="https://github.com/chahuja/pats" class="button">Dataset Scripts</a>
    <!-- <a href="https://github.com/chahuja/mix-stage" class="btn">View on GitHub</a>
    <a href="https://github.com/chahuja/mix-stage/zipball/master" class="btn">Download .zip</a>
    <a href="https://github.com/chahuja/mix-stage/tarball/master" class="btn">Download .tar.gz</a> -->
  </section>
  
  <section id="main-content" class="main-content" style="max-width: 75rem; min-width: 75rem; font-size: 0.9rem; background-color: white;">
    <h1>Abstract</h1>
    How can we teach robots or virtual assistants to gesture naturally? Can we go further and adapt the gesturing style to follow a specific speaker? Gestures that are naturally timed with corresponding speech during human communication are called co-speech gestures. A key challenge, called gesture style transfer, is to learn a model that generates these gestures for a speaking agent 'A' in the gesturing style of a target speaker 'B'. A secondary goal is to simultaneously learn to generate co-speech gestures for multiple speakers while remembering what is unique about each speaker. We call this challenge style preservation. In this paper, we propose a new model, named Mix-StAGE, which trains a single model for multiple speakers while learning unique style embeddings for each speaker's gestures in an end-to-end manner. A novelty of Mix-StAGE is to learn a mixture of generative models which allows for conditioning on the unique gesture style of each speaker. As Mix-StAGE disentangles style and content of gestures, gesturing styles for the same input speech can be altered by simply switching the style embeddings. Mix-StAGE also allows for style preservation when learning simultaneously from multiple speakers. We also introduce a new dataset, Pose-Audio-Transcript-Style (PATS), designed to study gesture generation and style transfer. Our proposed Mix-StAGE model significantly outperforms the previous state-of-the-art approach for gesture generation and provides a path towards performing gesture style transfer across multiple speakers. 
    
    <h1>Videos</h1>
    <h2 style="text-align:center">Teaser</h2>
    <center>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/xltrwH6sMC8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </center>

    <h1> Demo </h1>
    <p> <b> Navigating this demo:</b> The top left represents the style space of all the speakers in our pretrained model. 
      On the top right you will view can view the videos for both <b>Style Transfer</b> and <b>Style Preservation</b>. 
      Both the animations are generated by our model. The grid bottom can take you through all the styles in the model. 
      For example, if you choose <i>'lec_cosmic'</i> as the style and the <i>'0'th</i> audio in the <i>'maher'</i> column, the generated animation will be in the style of <i>'lec_cosmic'</i> but with the audio of <i>'maher'</i>. 
      You can choose from any style and any audio listed in the grid. 
      There are 7 different audios and 8 different styles to choose from.</p>
      <div class="img-container">      
        <img id="annotimage" class="img-fluid annotimage" src="figs/gesture_space_8sp_annotated.png" style="width: 510px; height: 390px">
        <!-- <div id="drawing" class="drawing-class">
          <svg id="SvgjsSvg1371" width="100%" height="100%" xmlns="http://www.w3.org/2000/svg" version="1.1" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:svgjs="http://svgjs.com/svgjs" viewBox="0 0 454 395.56" class="segmentation">
            <defs id="SvgjsDefs1372"></defs>
            <polygon id="SvgjsPolygon1373" points="0,0 100,0 0,100 0,0" fill-opacity="0.7" fill="#e1b9be" stroke-opacity="0.7" stroke="#e1b9be" stroke-width="1"></polygon>
          </svg> -->
        </div>
      </div>
      
      <div class="video-container">
        <p id="vid2" style="text-align: center; font-size: 1.6rem">Style Preservation</p>
        <p id="vid2style" style="text-align: center;">Style:</p>
        <p id="vid2audio" style="text-align: center;">Audio:</p>
        <div class="container">
          <video id="video2" width="420">
            <source id="src2" src="" type="video/mp4">
              Your browser does not support HTML video.
            </video>
          </div>
        </div>
        <div class="video-container">
          <p id="vid1" style="text-align: center; font-size: 1.6rem">Style Transfer</p>
          <p id="vid1style" style="text-align: center;">Style:</p>
          <p id="vid1audio" style="text-align: center;">Audio:</p>
          <div class="container">
            <video id="video1" width="420">
              <source id="src1" src="" type="video/mp4">
                Your browser does not support HTML video.
              </video>
            </div>
          </div>
          
          <ul class="donate-now" id="style", style="min-width: 65rem; max-width: 65rem;">
            <li><p name="style" style="text-align: center; font-size: 1.8rem;">Style</p></li>
          </ul>
          <br><br>
          <hr width=940 style="border-bottom: 2px solid black;">
          <div id="audio"></div>
          <hr width=940 style="border-bottom: 2px solid white;">
          <br>
          <br>

          <h1> Media Coverage</h1>
          <p align="center">
            <a href="https://techxplore.com/news/2020-08-mix-stage-gestures-accompany-virtual-agent.html" target="_blank"><img src="figs/logo_techxplore.png" width="200"></a>
            </font>
        </p>
          <h1> Acknowledgements </h1>
          <p align="left">
            This material is based upon work partially supported by the National Science Foundation (Awards #1750439 #1722822), National Institutes of Health and the InMind project. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of National Science Foundation or National Institutes of Health, and no official endorsement should be inferred.
          </p>

          <hr width=940 style="border-bottom: 2px solid black;">
          <footer class="site-footer">
            <span class="site-footer-owner"><a href="https://github.com/chahuja/mix-stage"> Style Transfer for Co-Speech Gesture Animation: A Multi-Speaker Conditional-Mixture Approach</a> is maintained by <a href="https://github.com/chahuja">chahuja</a>.</span>
            
            <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
          </footer>
          
        </section>
        <script src="https://code.jquery.com/jquery-1.9.1.min.js"></script>
        <script src="https://requirejs.org/docs/release/2.3.6/minified/require.js"></script>
        <script src="string.format.js"></script>
        <script>
          // current files
          var currStyle = "";
          var currAudio = "";
          var currFile = "";
          var audioBasePath = 'videos/'
          
          function getFileNames(basePath, key) {
            addHiddenFrame(key, "main-content");
            console.log(basePath)
            var files = new Array();
            $.ajax({url: basePath, success: function(data) 
              {
                $("#"+key).append(data);
                $("#" + key).each(function(){
                  $(" td > a").each(function(){
                    files.push($(this).text())
                  });
                  //console.log(files)
                  localStorage.setItem(key, JSON.stringify(files));
                });
              }});
            }
            
            function addHiddenFrame(id, parent_id) {
              //document.getElementById(id).remove()
              var elem = '<iframe id={0} class="hidden"></iframe>'.format(id)
              $('#'+parent_id).append(elem)  
            }
            
            function addAudioRow(id, parent_id) {
              var elem = '<ul class="donate-now" id="audio-{0}" style="min-width: 65rem; max-width: 65rem;"></ul>'.format(id)
              $('#'+parent_id).append(elem)
              // add audio name\
              $('#audio-'+id).append('<li><p name="audio" style="text-align: center; font-size: 1.6rem;">Audio-{0}</p></li>'.format(id))
            }
            
            function updateVideo(styleORaudio, speaker, filename) {
              console.log(styleORaudio)
              if (styleORaudio == 0){
                currStyle = speaker
              }
              else {
                currAudio = speaker
                currFile = filename
              }
              console.log(currStyle)
              console.log(currAudio)
              console.log(currFile)
              
              // modify the video files
              var folder;
              if (currStyle.localeCompare(currAudio) == 0){
                folder = 'render_eval/test/{0}'.format(currAudio) 
              }
              else
              {
                folder = 'render_eval_{0}_{1}/test/{0}'.format(currAudio, currStyle)
              }
              var vid1 = '{0}{1}/{2}'.format(audioBasePath, folder, currFile)
              
              folder = 'render_eval/test/{0}'.format(currAudio) 
              var vid2 = '{0}{1}/{2}'.format(audioBasePath, folder, currFile)
              console.log(vid1, vid2)
              
              // update the video elements
              document.getElementById("src1").src = vid1
              document.getElementById("src2").src = vid2
              
              //play the videos
              var vidObj1 = document.getElementById("video1");
              var vidObj2 = document.getElementById("video2");
              loadNplay(vidObj1, false)
              loadNplay(vidObj2, true)

              //modify the style and audio values above the animation
              document.getElementById("vid1style").innerHTML = 'Style: <span style="color:blue">'+ currStyle + '</span>'
              document.getElementById("vid1audio").innerHTML = 'Audio: <span style="color:blue">'+ currAudio + '</span>'
              document.getElementById("vid2style").innerHTML = 'Style: <span style="color:blue">'+ currAudio + '</span>'
              document.getElementById("vid2audio").innerHTML = 'Audio: <span style="color:blue">'+ currAudio + '</span>'
              
            }
            
            function loadNplay(vid, mute) { 
              vid.load();
              vid.muted = mute
              vid.play(); 
            } 
            
            $(document).ready(function () 
            {
              // Read the filenames
              var basePath = 'gesture_space_regions/';
              key_speakers = 'speakers'
              getFileNames(basePath, key_speakers)
              var speakers = JSON.parse(localStorage.getItem(key_speakers));  
              
              // Read the audio filenames
              $.each(speakers, function (index, value) {
                var style = value.split(".")[0];
                var audioPath = audioBasePath + 'render_eval/test/{0}/'.format(style);
                getFileNames(audioPath, style);
                
              })
              
              // edit the filenames
              speakers.reverse()
              $.each(speakers, function (index, value) {
                if (index < speakers.length -1){
                  var style = value.split(".")[0];
                  var style_old = speakers[index+1].split(".")[0];
                }
                else{
                  var style = value.split(".")[0];
                  var style_old = "speakers";
                }
                var audioFiles = JSON.parse(localStorage.getItem(style));
                var audioFiles_old = JSON.parse(localStorage.getItem(style_old));
                audioFiles = audioFiles.slice(audioFiles_old.length, audioFiles.length)
                localStorage.setItem(style, JSON.stringify(audioFiles));
              })
              
              // Create Style Buttons
              $.each(speakers, function(index, value){
                var style = value.split(".")[0]
                $("#style").append('<li><input type="radio" id="{0}" name="style" onclick="updateVideo(0, \'{1}\')"/><label for="{0}">{1}</label></li>'.format(index, style))
              });
              
              // Create Audio Buttons
              $.each(speakers, function(i, value){
                var style = value.split(".")[0]
                var audioFiles = JSON.parse(localStorage.getItem(style))
                $.each(audioFiles, function (j, value) {
                  if (j<7){
                    if (i == 0){
                      addAudioRow(j, "audio")
                    }
                    $("#audio-{0}".format(j)).append('<li><input type="radio" id="audio-{1}{0}" name="audio" onclick="updateVideo(1, \'{3}\', \'{2}\')"/><label for="audio-{1}{0}">{1}</label></li>'.format(i, j, value, style))
                  }
                })
              });
            });
            
          </script>  
          
        </body>
        </html>
        